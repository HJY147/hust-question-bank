"""
æœºå™¨å­¦ä¹ å¢å¼ºåŒ¹é…æ¨¡å—
ä½¿ç”¨æœºå™¨å­¦ä¹ ä¼˜åŒ–é¢˜ç›®åŒ¹é…å‡†ç¡®åº¦
"""
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import pickle
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
import json

from .config import config


class MLMatcher:
    """æœºå™¨å­¦ä¹ åŒ¹é…å¢å¼ºå™¨"""
    
    def __init__(self, model_dir: Optional[Path] = None):
        self.model_dir = model_dir or config.ML_MODEL_DIR
        self.model_dir.mkdir(parents=True, exist_ok=True)
        
        self.classifier = None
        self.scaler = StandardScaler()
        self.feature_weights = {
            'text_similarity': config.ML_TEXT_WEIGHT,
            'image_similarity': config.ML_IMAGE_WEIGHT,
            'category_match': 0.2,
            'length_ratio': 0.1
        }
        
        # åŠ è½½å·²è®­ç»ƒæ¨¡å‹
        self.load_model()
    
    def extract_features(self, 
                        query_embedding: np.ndarray,
                        candidate_embedding: np.ndarray,
                        query_category: str,
                        candidate_category: str,
                        query_length: int,
                        candidate_length: int) -> np.ndarray:
        """
        æå–ç‰¹å¾å‘é‡
        
        Args:
            query_embedding: æŸ¥è¯¢é¢˜ç›®çš„åµŒå…¥å‘é‡
            candidate_embedding: å€™é€‰é¢˜ç›®çš„åµŒå…¥å‘é‡
            query_category: æŸ¥è¯¢é¢˜ç›®ç±»åˆ«
            candidate_category: å€™é€‰é¢˜ç›®ç±»åˆ«
            query_length: æŸ¥è¯¢æ–‡æœ¬é•¿åº¦
            candidate_length: å€™é€‰æ–‡æœ¬é•¿åº¦
            
        Returns:
            ç‰¹å¾å‘é‡
        """
        features = []
        
        # 1. ä½™å¼¦ç›¸ä¼¼åº¦
        cosine_sim = np.dot(query_embedding, candidate_embedding) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
        )
        features.append(cosine_sim)
        
        # 2. æ¬§æ°è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰
        euclidean_dist = np.linalg.norm(query_embedding - candidate_embedding)
        features.append(1 / (1 + euclidean_dist))  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        
        # 3. ç±»åˆ«åŒ¹é…ï¼ˆone-hotï¼‰
        category_match = 1.0 if query_category == candidate_category else 0.0
        features.append(category_match)
        
        # 4. é•¿åº¦æ¯”ç‡
        length_ratio = min(query_length, candidate_length) / max(query_length, candidate_length)
        features.append(length_ratio)
        
        # 5. å‘é‡çš„ç»Ÿè®¡ç‰¹å¾
        features.append(np.mean(query_embedding))
        features.append(np.std(query_embedding))
        features.append(np.mean(candidate_embedding))
        features.append(np.std(candidate_embedding))
        
        # 6. å‘é‡å·®å¼‚
        diff = query_embedding - candidate_embedding
        features.append(np.mean(np.abs(diff)))
        features.append(np.max(np.abs(diff)))
        
        return np.array(features)
    
    def train(self, 
             training_data: List[Dict[str, Any]], 
             save_model: bool = True) -> Dict[str, float]:
        """
        è®­ç»ƒåŒ¹é…æ¨¡å‹
        
        Args:
            training_data: è®­ç»ƒæ•°æ®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
                {
                    'query_embedding': np.ndarray,
                    'candidate_embedding': np.ndarray,
                    'query_category': str,
                    'candidate_category': str,
                    'query_length': int,
                    'candidate_length': int,
                    'is_match': bool  # æ ‡ç­¾ï¼šæ˜¯å¦åŒ¹é…
                }
            save_model: æ˜¯å¦ä¿å­˜æ¨¡å‹
            
        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        if len(training_data) < config.ML_MIN_SAMPLES:
            return {
                'error': f'è®­ç»ƒæ ·æœ¬ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {config.ML_MIN_SAMPLES} ä¸ªæ ·æœ¬',
                'samples': len(training_data)
            }
        
        # æå–ç‰¹å¾å’Œæ ‡ç­¾
        X = []
        y = []
        
        for sample in training_data:
            features = self.extract_features(
                sample['query_embedding'],
                sample['candidate_embedding'],
                sample['query_category'],
                sample['candidate_category'],
                sample['query_length'],
                sample['candidate_length']
            )
            X.append(features)
            y.append(1 if sample['is_match'] else 0)
        
        X = np.array(X)
        y = np.array(y)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # è®­ç»ƒæ¢¯åº¦æå‡åˆ†ç±»å™¨
        self.classifier = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        
        self.classifier.fit(X_train_scaled, y_train)
        
        # è¯„ä¼°æ¨¡å‹
        train_score = self.classifier.score(X_train_scaled, y_train)
        test_score = self.classifier.score(X_test_scaled, y_test)
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = self.classifier.feature_importances_
        
        metrics = {
            'train_accuracy': float(train_score),
            'test_accuracy': float(test_score),
            'samples': len(training_data),
            'feature_importance': feature_importance.tolist()
        }
        
        # ä¿å­˜æ¨¡å‹
        if save_model:
            self.save_model(metrics)
        
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ:")
        print(f"  è®­ç»ƒå‡†ç¡®ç‡: {train_score:.4f}")
        print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_score:.4f}")
        
        return metrics
    
    def predict_similarity(self,
                          query_embedding: np.ndarray,
                          candidate_embedding: np.ndarray,
                          query_category: str,
                          candidate_category: str,
                          query_length: int,
                          candidate_length: int) -> float:
        """
        é¢„æµ‹åŒ¹é…ç›¸ä¼¼åº¦
        
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if self.classifier is None:
            # å¦‚æœæ¨¡å‹æœªè®­ç»ƒï¼Œä½¿ç”¨ç®€å•ä½™å¼¦ç›¸ä¼¼åº¦
            return float(np.dot(query_embedding, candidate_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(candidate_embedding)
            ))
        
        # æå–ç‰¹å¾
        features = self.extract_features(
            query_embedding,
            candidate_embedding,
            query_category,
            candidate_category,
            query_length,
            candidate_length
        )
        
        # æ ‡å‡†åŒ–
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        
        # é¢„æµ‹æ¦‚ç‡ï¼ˆç±»åˆ«1çš„æ¦‚ç‡å³ä¸ºç›¸ä¼¼åº¦ï¼‰
        similarity = self.classifier.predict_proba(features_scaled)[0][1]
        
        return float(similarity)
    
    def save_model(self, metrics: Optional[Dict] = None):
        """ä¿å­˜æ¨¡å‹åˆ°ç£ç›˜"""
        model_path = self.model_dir / 'ml_matcher.pkl'
        scaler_path = self.model_dir / 'scaler.pkl'
        metrics_path = self.model_dir / 'metrics.json'
        
        with open(model_path, 'wb') as f:
            pickle.dump(self.classifier, f)
        
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        if metrics:
            with open(metrics_path, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
    
    def load_model(self) -> bool:
        """ä»ç£ç›˜åŠ è½½æ¨¡å‹"""
        model_path = self.model_dir / 'ml_matcher.pkl'
        scaler_path = self.model_dir / 'scaler.pkl'
        
        if not model_path.exists() or not scaler_path.exists():
            print("â„¹ï¸  æœªæ‰¾åˆ°å·²è®­ç»ƒçš„MLæ¨¡å‹ï¼Œå°†ä½¿ç”¨åŸºç¡€ç›¸ä¼¼åº¦è®¡ç®—")
            return False
        
        try:
            with open(model_path, 'rb') as f:
                self.classifier = pickle.load(f)
            
            with open(scaler_path, 'rb') as f:
                self.scaler = pickle.load(f)
            
            print(f"âœ… MLæ¨¡å‹å·²åŠ è½½: {model_path}")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False


def generate_training_data_from_db(db_path: str, 
                                   num_positive: int = 100,
                                   num_negative: int = 300) -> List[Dict]:
    """
    ä»æ•°æ®åº“ç”Ÿæˆè®­ç»ƒæ•°æ®
    
    Args:
        db_path: æ•°æ®åº“è·¯å¾„
        num_positive: æ­£æ ·æœ¬æ•°é‡ï¼ˆç›¸åŒé¢˜ç›®ï¼‰
        num_negative: è´Ÿæ ·æœ¬æ•°é‡ï¼ˆä¸åŒé¢˜ç›®ï¼‰
        
    Returns:
        è®­ç»ƒæ•°æ®åˆ—è¡¨
    """
    import sqlite3
    
    training_data = []
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # è·å–æ‰€æœ‰é¢˜ç›®
    cursor.execute("""
        SELECT question_id, category, question_text, text_embedding
        FROM questions
    """)
    
    questions = []
    for row in cursor.fetchall():
        q_id, category, text, embedding_blob = row
        embedding = np.frombuffer(embedding_blob, dtype=np.float32)
        questions.append({
            'id': q_id,
            'category': category,
            'text': text,
            'embedding': embedding,
            'length': len(text)
        })
    
    conn.close()
    
    # ç”Ÿæˆæ­£æ ·æœ¬ï¼ˆåŒä¸€é¢˜ç›®çš„å˜ä½“ï¼Œè¿™é‡Œç”¨é¢˜ç›®è‡ªèº«ä½œä¸ºæ­£æ ·æœ¬ï¼‰
    for i, q in enumerate(questions[:num_positive]):
        training_data.append({
            'query_embedding': q['embedding'],
            'candidate_embedding': q['embedding'],
            'query_category': q['category'],
            'candidate_category': q['category'],
            'query_length': q['length'],
            'candidate_length': q['length'],
            'is_match': True
        })
    
    # ç”Ÿæˆè´Ÿæ ·æœ¬ï¼ˆä¸åŒç±»åˆ«çš„é¢˜ç›®å¯¹ï¼‰
    import random
    for _ in range(num_negative):
        q1, q2 = random.sample(questions, 2)
        # ç¡®ä¿ç±»åˆ«ä¸åŒ
        if q1['category'] == q2['category']:
            continue
        
        training_data.append({
            'query_embedding': q1['embedding'],
            'candidate_embedding': q2['embedding'],
            'query_category': q1['category'],
            'candidate_category': q2['category'],
            'query_length': q1['length'],
            'candidate_length': q2['length'],
            'is_match': False
        })
    
    print(f"ğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®: {len(training_data)} ä¸ªæ ·æœ¬")
    return training_data


if __name__ == '__main__':
    # æµ‹è¯•MLåŒ¹é…å™¨
    from backend.config import Config
    
    Config.print_status()
    
    # è®­ç»ƒæ¨¡å‹ç¤ºä¾‹
    db_path = Config.DB_PATH
    if db_path.exists():
        print("\nå¼€å§‹è®­ç»ƒMLæ¨¡å‹...")
        training_data = generate_training_data_from_db(str(db_path))
        
        matcher = MLMatcher()
        metrics = matcher.train(training_data)
        
        print("\nè®­ç»ƒå®Œæˆï¼")
        print(json.dumps(metrics, indent=2, ensure_ascii=False))
